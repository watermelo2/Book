现今很多应用程序都是 数据密集型(data-intensive) 的,而非 计算密集型(compute-intensive)的. 
因此CPU很少成为这类应用的瓶颈,更大的问题通常来自数据量、数据复杂性、以及数据的变更速度.

#### 可靠性、可扩展性、可维护性
可靠性(Reliability): 系统在困境(adversity)(硬件故障、软件故障、人为错误)中仍可正常工作(正确完成功能,并能达到期望的性能水准)
可扩展性(Scalability): 有合理的办法应对系统的增长(数据量、流量、复杂性). 讨论可扩展性意味着考虑诸如“如果系统以特定方式增长，
					  有什么选项可以应对增长？”和“如何增加计算资源来处理额外的负载？”等问题。
可维护性(Maintainability): 许多不同的人(工程师、运维)在不同的生命周期,都能高效地在系统上工作(使系统保持现有行为,并适应新的应用场景)

#### Twitter推送系统
1. 第一版: SQL关联查询
2. 第二版: 提前计算、存储每个用户需要查看的关注的人的推文收件箱
  		  消耗计算: 平均一条推文平均会发送75个关注者,所以每秒4.6K的发推写入,变成每秒345K的写入
  		  缺点: 有的用户有超过3000w的粉丝,他们发推文会导致3000w次的写入!!!
3. 第三版: 两种方法的混合。大多数用户发的推文会被扇出写入其粉丝主页时间线缓存中。但是少数拥有海量粉丝的用户（即名流）
		  会被排除在外。当用户读取主页时间线时，分别地获取出该用户所关注的每位名流的推文，再与用户的主页时间线缓存合并，
		  如第一版所示.

#### 延迟和响应时间的区别
延迟（latency） 和 响应时间（response time）经常用作同义词，但实际上它们并不一样。响应时间是客户所看到的，除了实际处理
请求的时间（ 服务时间（service time） ）之外，还包括网络延迟和排队延迟。延迟是某个请求等待处理的持续时长，在此期间它处于
休眠（latent） 状态，并等待服务

TODO 表结构的规范化和非规范化

关系模型的一个关键洞察是: 只需构建一次查询优化器,随后使用该数据库的所有应用程序都可以从中受益. 如果你没有查询优化器的话,那么为
特定查询手动编写访问路径比编写通用优化器更加容易--不过从长期看通用解决方案更好

声明式查询语言适合并行执行,命令式查询语言并不是特别合适

#### 两大类存储引擎: 日志结构(log-structured)的存储引擎和面向页面(page-oriented)的存储引擎

面向页面: B树
日志结构: asd

#### SSTables(Sorted String Table)和LSM树

SSTables:
	存储: 按键排序、归档、压缩
	搜索: 存储时按规则分块 + 内存字节扫描.   如果字符长度是定长(通常不是),那么可以用二分查找.因为定长的话可以知道每条记录的分界点
										  (前一条记结束,后一条记录开始的地方)
LSM树的基本思想: 保存一系列在后台合并的SSTables,简单而有效。即使数据集比可用内存大得多，它仍能继续正常工作。由于数据按排序顺序存储，
因此可以高效地执行范围查询（扫描所有高于某些最小值和最高值的所有键），并且因为磁盘写入是连续的，所以LSM树可以支持非常高的写入吞吐量

WAL(write-ahead-log)在MySQL中叫"重做日志"(redo log)

B树有个优点是每个键只存在于索引中的一个位置,而日志结构化的存储引擎可能在不同的段中有相同键的多个副本. 这意味着在实现事务语义时方便
太多了: 在许多关系型数据库中,事务隔离是通过在键范围上使用锁来实现的.

反直觉的是，内存数据库的性能优势并不是因为它们不需要从磁盘读取的事实。即使是基于磁盘的存储引擎也可能永远不需要从磁盘读取，
因为操作系统缓存最近在内存中使用了磁盘块。相反，它们更快的原因在于省去了将内存数据结构编码为磁盘数据结构的开销

列存储适用于OLAP是因为分析类型的查询语句中通常不会像OLTP一样的拿非常多行,并且OLAP对应的数据仓库中的数据非常之巨大,所以如果用
行存储的话性能简直不敢想象的低,一个查询语句下去可能会把内存占满.  还有就是OLAP常用统计函数,这种情况下列存储在硬盘中的数据摆放都是
顺序的,顺序读比随机读快一个量级(参考4KB顺序读和4KB随机读测速). 还有,列存储压缩数据会比较简单.

列族: 在每个列族中,它们将一行中的所有列与行键一起存储,并且不使用压缩.

列存储索引和行存储索引区别: 行索引中的多个索引(二级索引)的存储将每一行保存在一个地方(聚簇索引或堆文件); 列索引直接存值;

LSM树的思路同样适用于行存储和列存储

## 编码

XML和CSV不能区分数字和字符串. JSON虽然能区分,但是不区分整数和浮点数,而且不能指定精度.
XML和JSON对Unicode字符串有很好的支持,但是它们不支持二进制数据字符编码. 通常会进行Base64后传输,但会增加约33%的数据大小.

PS: 让不同的组织达成一致的难度超过了其它大多数的问题

Thrift的CompactProtocol(三种二进制协议中的一种)还算有趣,虽然都是些别的地方已经有的编码思路
Protobuf在对数组|列表进行编码时和其它协议不一样,它是通过重复字段的重复标记来实现的. 它有个好处,可以将单值改为多值字段,它是兼容的
Avro是一种之前没有见过的编码思路. 附一张图: https://s2.loli.net/2022/06/02/FclZpVqRtP8SiX6.png

我的看法: 越是扣字节的编码兼容起来越麻烦(Avro),但性能非常好

SOAP优点: 给Producer/Consumer双方提供了契约(对于微服务来说很喜欢这个,有专门的契约测试)、安全调用、事务(两阶段)

本地函数调用和RPC的区别: 可预测性、异常原因、幂等、执行时间、数据编码(大对象)、代码优化(我自己想到的)

asd


分布式搜索引擎中的服务树的概念(目的是为了能够横向扩展,每台示例能够执行任务树中的一个子节点树)

```
DocId: 10 	r1
Links 
  Forward: 20 
  Forward: 40   Forward: 60 
Name  
  Language  
    Code: 'en-us'     Country: 'us'     
Language 
    Code: 'en' 
  Url: 'http://A' 
Name 
  Url: 'http://B' 
Name 
  Language 
    Code: 'en-gb'     Country: 'gb'


# Name.Language.Code
# value  r  d
# en-us  0  2  -- r=0是因为没有重复
# en     2  2  -- r=2是因为`Name.Language`重复,所以可以认为级别就是层级
# NULL   1  1  -- 需要加这层的原因是为了表明下面的en-gn是在第三个Name中而不是第二个
# en-gn  1  2  -- r=1是因为`Name`重复
# NULL   0  1

# r: 重复级别  d: 定义级别

```

任何小于字段路径中重复字段和可选字段数的definition level都表示 NULL

# 将一条记录分解为若干列的算法实现
```
procedure DissectRecord(RecordDecoder decoder, FieldWriter writer, int repetitionLevel):
  Add current repetitionLevel and definition level to writer
  seenFields = {} // empty set of integers
  while decoder has more field values
 	FieldWriter chWriter =
		child of writer for field read by decoder
 	int chRepetitionLevel = repetitionLevel
 	if set seenFields contains field ID of chWriter
 		chRepetitionLevel = tree depth of chWriter
 	else
 		Add field ID of chWriter to seenFields
 	end if
 	if chWriter corresponds to an atomic field
 		Write value of current field read by decoder
 		using chWriter at chRepetitionLevel
 	else
 		DissectRecord(new RecordDecoder for nested record 
 				read by decoder, chWriter, chRepetitionLevel)
 	end if
 end while
end procedure
```


```
-- 查询SQL
SELECT DocId AS Id,
	COUNT(Name.Language.Code) WITHIN Name AS Cnt,
	Name.Url + ',' + Name.Language.Code AS Str
FROM t
WHERE REGEXP(Name.Url, '^http') AND DocId < 20 ;


-- 查询结果
Id: 10
Name
	Cnt: 2
	Language
		Str: 'http://A,en-us'
		Str: 'http://A,en'
Name
	Cnt: 0		

-- 输出schema
message QueryResult{
	require int64 Id;
	repeated group Name{
		optional uint64 Cnt;
		repeated group Language{
			optional string Str;
		}
	}
}

-- 算法实现
procedure ConstructFSM(Field[] fields):
for each field in fields:
	maxLevel = maximal repetition level of field
	barrier = next field after field or final FSM state otherwise
 	barrierLevel = common repetition level of field and barrier
 	for each preField before field whose
 			repetition level is larger than barrierLevel:
 		backLevel = common repetition level of preField and field
 		Set transition (field, backLevel) -> preField
 	end for
 	for each level in [barrierLevel+1..maxLevel]
 			that lacks transition from field:
 		Copy transition's destination from that of level-1
 	end for
 	for each level in [0..barrierLevel]:
 		Set transition (field, level) -> barrier
 	end for
end for
end procedure
```

列式存储在写入数据时非常麻烦,如果想到排序表中插入一行,可能导致重写所有的列文件. 一个好的办法是LSM树

- 日志结构学派
> 只允许附加到文件和删除过时的文件,但不会更新已经写入的文件. Bitcask,SSTables,LSM树,LevelDB,Cassandra,HBase,Lucene等都属于这个类别.

- 就地更新学派
> 将磁盘视为一组可以覆写的固定大小的页面. B树是这种哲学的典范,用在所有主要的关系数据库中和许多非关系型数据库.

Java内置的序列化性能在现在的版本中还是不是个问题
复制的三种流行的变更复制算法: 单领导者、多领导者、无领导者
最终一致性的保证
WAL的优势是仅仅追加字节序列,缺点是数据通常非常底层,比如: 包含哪些磁盘块中的哪些字节发生了更改. 再比如: 复制协议版本不匹配,需要停机做版本更新.  基于行的逻辑日志复制在这方面更有优势. 还有一种就是基于触发器的复制,它相对来说更加的灵活,但开销也更大
解决Leader写入单点问题的一个方法是: 允许多个写入节点,复制还是原来的逻辑(写入的每个节点都必须将该数据更改转发给所有其他节点). 这种方法称为"多领导配置". 在单数据中心的场景下,这种方式的好处很少超过复杂性的代价(会有非常多的不同场景的数据冲突问题)
多数据中心会碰到个大的缺点: 写冲突
多主复制的另一种适用场景是：应用程序在断网之后仍然需要继续工作(比如钉钉的在线文档,无网络时本地代表的就是一个数据库)
多主复制在同时编辑一条数据时处理起来不如单主,"协同编辑"场景就是个例子
冲突的解决通常适用于单个行或文档层面,而不是整个事务
Redis和Dynamo的高可用实现: Gossip
三种可设置的领导者复制的拓扑结构: 环型、星型、All to All型
Dynamo的无主复制(Leaderless)不适用于Amazon以外的用户,AWS对外提供的DynamoDB是用的"单引导程序复制"(这个没搜到相关资料)
多主拓扑中数据修复经常使用`读修复`和`反熵过程`两种机制,`读修复`在监控陈旧度时非常的困难(无法做限制)
能够量化"最终一致性"中的"最终"很重要
关于"并发"的定义: 如果两个操作都意识不到对方的存在,就称这两个操作并发,而不管它们发生的物理时间.人们有时把这个原理和狭义相对论的物理学联系起来,它引入了信息不能比光速更快的思想.因此,如果事件之间的时间短于光通过它们之间的距离,那么发生一定距离的两个事件不可能相互影响.
传统的复制型关系数据库系统都将关注点放在保证副本的强一致性. 虽然强一致性可以给应用的写入操作提供方便的编程模型,但导致系统的扩展
性和可用性非常受限,无法处理网络分裂的情况
每个key会被分配一个coordinator节点,它负责落到它管理的范围内的数据的复制. 它除了自己存储一份之外,还会在环上顺时针方向的
其它N-1个节点存储一份副本. 因此在系统中,每个节点要负责从它自己往后的一共N个节点. 另外,由于一致性哈希算法的问题,它会
跳过一些位置,以保证list里面的节点都在不同的物理节点上. 不过这种做法在网络分区时还是避免不了在限定时间内传递到所有副本的
问题

一致性哈希算法会丢失高效执行"范围查询"能力. 具体到数据库来说,如果在MongoDB中使用了基于散列的分区模式,则任何范围查询都
必须发送到所有分区; Riak、Couchbase等数据库不支持主键上的范围查询; Cassandra采用了折中的策略: 表可以使用由多个列组成
的复合主键来声明.键中只有第一列会作为散列的依据,而其他列则被用作Casssandra的SSTables中排序数据的连接索引.尽管查询无
法在复合主键的第一列中按范围扫表,但如果第一列已经指定了固定值,则可以对该键的其他列执行有效的范围扫描

一种简单处理热Key的方法: 在主键的开始或结尾添加一个随机数,只要一个两位数的十进制随机数就可以将主键分散为100种不同的主键,
从而存储在不同的分区中. 将主键进行分割之后,任何读取都必须要做额外的工作,因为他们必须从所有100个主键分布中读取数据并将
其合并.此技术还需要额外的记录：只需要对少量热点附加随机数;对于写入吞吐量低的绝大多数主键来是不必要的开销.因此,您还需要
一些方法来跟踪哪些键需要被分割

HBase为了减少实现的复杂度而放弃了次级索引

有两种对二级索引数据库进行分区的方法: 基于文档的分区和基于关键词的分区
基于文档: 存在多个相同值的二级索引字段被分散到多个分区的问题(因为它们是通过文档ID来进行分区的),这样通过这个索引检索的时候
		  需要将查询发送到所有分区并合并返回结果. MongoDB、Riak、Cassandra、Elasticsearch、SolrCloud、VoltDB都是这么干的
基于关键字: 构建了一个能覆盖所有分区数据全局二级索引,并将其按规则(顺序)分散到各个分区. 优点是查询的时候能明确知道去哪几个
			分区; 缺点是写入时复杂,因为写入单个文档可能会影响索引的多个分区(有多个关键词,可能会位于不同的分区节点上),
			而且这还需要跨分区的事务支持

			在实践中,对全局二级索引的更新通常是异步的(也就是说,如果在写入之后不久读取索引,刚才所做的更改可能尚未反映
			在索引中).例如,Amazon DynamoDB声称在正常情况下,其全局次级索引会在不到一秒的时间内更新,但在基础架构出现
			故障的情况下可能会有延迟【20】	
HBase的底层是HDFS
一致性哈希: TODO 

全自动重新平衡可以很方便,因为正常维护的操作工作较少.但是,这可能是不可预测的.再平衡是一个昂贵的操作,因为它需要重新路由
请求并将大量数据从一个节点移动到另一个节点.如果没有做好,这个过程可能会使网络或节点负载过重,降低其他请求的性能

Eureka是AP的??? http://jasonwilder.com/blog/2014/02/04/service-discovery-in-the-cloud/

Cassandra和Riak采取不同的方法：他们在节点之间使用流言协议(gossip protocol) 来传播群集状态的变化. 请求可以发送到任意节点,
该节点会转发到包含所请求的分区的适当节点(图6-7中的方法1).这个模型在数据库节点中增加了更多的复杂性,但是避免了对像ZooKeeper
这样的外部协调服务的依赖.

ACID中的C的概念: 对数据的一组特定陈述必须始终成立,即"不变量". 所以这个C是应用程序的属性,不是数据库的属性

在CouchDB,Datomic和LMDB中使用另一种方法.虽然它们也使用B树,但它们使用的是一种仅追加/写时拷贝(append-only/copy-on-write) 的变体,它们在更新时不覆盖树的页面,而为每个修改页面创建一份副本.从父页面直到树根都会级联更新,以指向它们子页面的新版本.任何不受写入影响的页面都不需要被复制,并且保持不变.
使用仅追加的B树,每个写入事务(或一批事务)都会创建一颗新的B树,当创建时,从该特定树根生长的树就是数据库的一个一致性快照.没必要根据事务ID过滤掉对象,因为后续写入不能修改现有的B树；它们只能创建新的树根.但这种方法也需要一个负责压缩和垃圾收集的后台进程.

并发问题的测试是很难的,因为它们通常是非确定性的 —— 只有在倒霉的时机下才会出现问题.  哈哈

可序列化级别大部分使用了三种技术:
1. 字面意义上地串行顺序执行事务
	1.1. Redis;  
2. 两相锁定(2PL, two-phase locking),几十年来唯一可行的选择
	2.1. 读时阻塞写,写时阻塞读和写; 其实就是排他锁和共享锁
3. 乐观并发控制技术,例如可序列化的快照隔离
	3.1 乐观锁是一种
	3.2 序列化快照隔离(serializable snapshot isolation)非常有趣,它是根据"快照读隔离"所带来的"幻读"问题而提出的解决办法,
		即"记录每个活跃事务所读取的记录并在其它事务提交后标记被影响到的记录已经过时",听起来就是件开销非常大的事. 它与SPL相比
		的优点就是不需要阻塞. 缺点是对于长时间读取和写入数据的事务很可能会发生冲突并中止更加敏感.


另外,现代的存储过程放弃了PL/SQL而是用了现有的编程语言(Datomic使用Java或Clojure),Redis的Lua也是存储过程. 
"Gap Lock+Next Key Lock"是一个简易的"谓词锁"(predicate lock)实现: 适用于数据库中尚不存在,但将来可能会添加的
对象(幻象).如果两阶段锁定包含谓词锁,则数据库将阻止所有形式的写入偏差和其他竞争条件,因此其隔离实现了可串行化


装有良好软件的个人计算机通常要么功能完好,要么完全失效,而不是介于两者之间。 在分布式系统中,我们不再处于理想化的
系统模型中,我们别无选择,只能面对现实世界的混乱现实。而在现实世界中,各种各样的事情都可能会出现问题.

当错误处理策略由简单的放弃组成时,一个大的系统最终会花费大量时间从错误中恢复,而不是做有用的工作

异步网络具有无限的延迟限制

ISDN网络(电路交换网络)属于同步、固定带宽、频率运行(每秒4000帧,每帧16bit),它可以保证"有限延迟"; 而以太网和IP是分组交换协议,
它们是可以从队列中获得,它们针对"突发流量"进行了优化


# Dynamo

解决数据冲突会带来两个额外问题: 何时解决? 谁来解决?
Dynamo针对的主要是需要"永远可写"数据仓库的应用,并且它的"延迟敏感"使得它不能像Chord和Pastry一样使用多跳路由.  
Dynamo可以被描述为: 一个零跳(zero hop)分布式哈希表(DHT)
在发生故障并且存在并发更新的场景下,版本会发生分叉
vector clock有一个潜在问题: 如果有多个节点先后参与同一个对象的写操作,那这个对象的clock vector会变得很长. 但在实际中这
不太可能发生,因为写操作只会发生在perference list(存key的值的物理节点列表)的前N个节点中的一个来执行. 只有在网络分裂或多台
服务器挂掉的情况下,写操作才可能由非preference list前N个节点来执行,导致vector clock变长. Dynamo采用了一种vector clock截断
方案: 另外保存一个和(node, counter)对应的时间戳,记录对应的节点最后一次更新该记录的时间.当vector clock里的(node, counter) 数量达到一个阈值(例如,10)时,就删除最老的一项. 这样会带来无法精确判断部分后代的因果关系的问题.
Merkle Tree缺点: 每当有节点加入或离开系统时,一些key range会变,因此对应的tree需要重新计算
判断不可达: 节点B只要没有应答节点A的消息,A就可以认为B不可达,即便B可以应答C的消息
TODO 去中心化故障检测协议使用简单的 gossip 风格协议,使得系统内的每个节点都可以感知 到其他节点的加入或离开
写请求由preference list内的前N个节点中的任意一个参与者,好处是可以保证写入的顺序化,坏处是会导致不均匀的负载分布,损害SLA
为了解决这个问题,preference list内的所有N个节点都可以参与写操作,而且为一个写操作之前通常有一个读操作,因此写操作的参与者 
都选择为: 前一次读操作返回最快的那个节点,这个信息存储在读操作返回的上下文中

几个不同的使用场景做出不同处理:
1. 业务逻辑相关的: 每个数据都会复制到不同的节点上,发生版本冲突时由应用执行自己的reconciliation逻辑. 购物车服务就是这种场景--应用
				  自己来合并冲突的购物车版本
2. 基于时间戳的: 就是LWW(Last Write Wins). 维护用户Session信息适用于这种场景
3. 高性能读取引擎: 读多写少,一般配置R为1,W为N,保证数据权威(可用)性. 维护产品目录、促销商品服务会用到这种配置 

两种会出现数据版本分叉的情况:
1. 节点失败、数据中心故障或网络分裂等故障场景
2. 同一数据对象的大量并发写操作,不同节点都在coordinating操作

并发写数据上升通常都是自动化客户端程序(busy robots)导致的,极少是人(的应用)导致的.

客户端驱动或服务端驱动的 Coordination:

服务端驱动:
  客户请求会通过负载均衡器均匀地分发给哈希环上的所有节点.每个节点都可以作为读请求的coordinator,而写操作的coordinator必须
由key的preference list里面的节点才能充当.有这种限制是因为,preference list中的这些节点被赋予了额外的职责：创建一个
新的版本戳(version stamp),在因果关系上包含被它的写操作更新的版本.注意,如果Dynamo的版本化方案使用的是物理时间戳
(physical timestamps),那任何节点都可以coordinate写操作.  (大西瓜注: 钱在这个时候的作用非常大,比如谷歌用的原子钟+NTP)

客户端驱动:
  在这种方式中,客户端应用使用库(library)在本地执 行请求 coordination.每个客户端定期地随机选择一个 Dynamo 
节点,下载它的系统成员 状态(Dynamo membership state)的当前视图(current view).有了这个信息,客户端 就可以
知道任何 key 对应的 preference list 由哪些节点组成.

  读请求可以在客户端节点(client node)coordinate,因此如果请求是被负载均衡器随机 分给一个 Dynamo 
节点,那这种方式可以避免额外的网络转发跳数.写操作或者转发给 key 对应的 preference list里面的一个节点,
或者,如果使用的是基于时间戳的版本化方式 ,可以在本地 coordinate.

 客户端驱动的一个重要优势是：不再需要一个负载均衡器才能均匀地分发客户负载. 在存储节点上近乎均匀分布的 key,暗含了
 (implicitly guaranteed)负载的均匀分布.

  显然,这种方式的效率取决于客户端侧的成员信息的新鲜程度(how fresh the membership information).当前,每个客户端
会每隔10s随机地轮询(poll)一个Dynamo节点,获取成员更新(membership updates).这里选用pull而不是push
模型是考虑前者在大 量客户端的情况下可扩展性更好,而且相比于客户端侧,只需在服务端侧维护很少的状态信 息.

  然而,在最差的情况下,客户端的 membership 信息会有 10s 的脏数据. 因此,如果客户端检测到它的成员表
(membership table)过期了(例如,当一些成员不可 达的时候),它会立即更新它的成员信息

它的读时修复可以解决线性一致性和法定人数下因为网络延迟原因导致的客户端读取数据不一致(不正确)问题.
Writer向Replica1、Replica2、Replica3写入数据,但由于网络原因,Reader1和Reader读取数据时只有Replica1写好了最新值1,
其余的两个节点还是旧值0,Reader1读到了Replica1,Reader2读到了Replica2、Replica3,此时Reader1和Reader2的值不一致.

读时修复会在读取数据给应用之前同步执行修复.

顺序一致性和线性一致性的区别是什么?

CAP相关: https://web.archive.org/web/20220126012236/https://zhuanlan.zhihu.com/p/55053121

虽然因果是个很重要的概念,但实际上如果跟踪所有的因果关系是不切实际的. 
兰伯特时间(Lamport clock)适合事后确定全序关系

全序广播非正式地讲需要满足两个安全属性:
1. 可靠交付:
没有消息丢失,如果消息被传递到一个节点,它将被传递到所有节点

2. 全序交付:
消息以相同的顺序传播给每一个节点

全序网路在机器出现故障时也要保证机器恢复后消息能够及时通过并送达

分布式共识常见问题场景:
1. 领导选举
2. 原子提交

2PC都会有存疑事务的问题

纠删码允许比完全复制更低的存储开销以恢复丢失的数据. 这些技术与RAID相似,可以在连接到同一台机器的多个磁盘上提供冗余; 区别在于
在分布式系统中,文件访问和复制是在传统的数据中心网络上完成的,没有特殊的硬件

"OLAP"和"OLTP"和"批处理工作流"和"流处理"的作用是不同的

在使用分布式存储的时候应当尽量避免使用排序

Spark、Flink、Tez是用来解决MapReduce中的"数据连接"的解决方案在一些比较频繁遇到的场景下的一些缺点(是MapReducer模型导致的,所以
是通病)而新开发出来的,它们的设计方式有很多区别,但有一个共同点: 把整个工作流作为单个作业来处理,而不是把它分解为独立的子作业

"批处理"的输入数据有界,所以它总是知道自己是什么时候完成了一个作业

为了区分不同的消息系统,需要思考两个问题:
1. 如果生产者发送消息的速度比消费者能够处理的速度块会发生什么
2. 如果节点崩溃或暂时脱机,会发生什么情况

保持多数据库系统的数据同步问题解决方法可以用"双写",但"双写"还需要引入2PC(有可能"单失败"),所以CDC(change data capture)出来了

事件在用于表示用户的操作意图时是不可压缩的,而用于表示因为操作而发生的状态更新时是可以压缩的

在事件朔源系统中真正删除数据是非常苦难的,因为副本可能存在于很多地方. 删除更多的是“使取回数据更困难”,而不是“使取回数据不可能”。

【复合事件处理】
CEP(complex,event processing)是为分析事件流而开发出的一种方法,CEP系统通常使用高层次的声明式查询语言,比如SQL,或者图形用户
界面,来描述应该检测到的事件模式。这些查询被提交给处理引擎,该引擎消费输入流,并在内部维护一个执行所需匹配的状态机。当发现匹配时,
引擎发出一个复合事件(complex event)(因此得名),并附有检测到的事件模式详情.  在这些系统中,查询和数据之间的关系与普通数据库
相比是颠倒的。通常情况下,数据库会持久存储数据,并将查询视为临时的：当查询进入时,数据库搜索与查询匹配的数据,然后在查询完成时丢
掉查询。 CEP引擎反转了角色：查询是长期存储的,来自输入流的事件不断流过它们,搜索匹配事件模式的查询

使用流处理的另一个领域是对流进行分析。 CEP与流分析之间的边界是模糊的,但一般来说,分析往往对找出特定事件序列并不关心,
而更关注大量事件上的聚合与统计指标. Apache Storm,Spark Streaming,Flink都是针对分析设计的

函数幂等同样可以实现"恰好一次语义",即: 再次写入该值,只是用同样的值替代

衍生数据与分布式事务最大的区别在于: 事务系统通常提供"线性一致性",这包含有用的保证,例如"读之已写". 另一方面,衍生数据系统通常是异步
更新的,因此默认不会提供相同的时序保证(即不保证"线性一致性")

在Lambda架构中建议并行运行两个不同的系统: 批处理系统和独立的流系统. 流处理器消耗事件并快速生成对视图的近似更新,批处理器稍后将
使用同一组事件并生成衍生视图的更正版本. 这个设计背后的原因是批处理更简单,因此不易出错,而流处理被认为是不太可靠和难以容错的.
而且,流处理可以使用快速近似算法,而批处理使用较慢的精确算法。

违反及时性,"最终一致性"; 违反完整性,"永无一致性"

系统自我验证、自我审计值得投资


 


