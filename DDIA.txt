
```
DocId: 10 	r1
Links 
  Forward: 20 
  Forward: 40   Forward: 60 
Name  
  Language  
    Code: 'en-us'     Country: 'us'     
Language 
    Code: 'en' 
  Url: 'http://A' 
Name 
  Url: 'http://B' 
Name 
  Language 
    Code: 'en-gb'     Country: 'gb'


# Name.Language.Code
# value  r  d
# en-us  0  2  -- r=0是因为没有重复
# en     2  2  -- r=2是因为`Name.Language`重复,所以可以认为级别就是层级
# NULL   1  1  -- 需要加这层的原因是为了表明下面的en-gn是在第三个Name中而不是第二个
# en-gn  1  2  -- r=1是因为`Name`重复
# NULL   0  1

# r: 重复级别  d: 定义级别

```

任何小于字段路径中重复字段和可选字段数的definition level都表示 NULL

# 将一条记录分解为若干列的算法实现
```
procedure DissectRecord(RecordDecoder decoder, FieldWriter writer, int repetitionLevel):
  Add current repetitionLevel and definition level to writer
  seenFields = {} // empty set of integers
  while decoder has more field values
 	FieldWriter chWriter =
		child of writer for field read by decoder
 	int chRepetitionLevel = repetitionLevel
 	if set seenFields contains field ID of chWriter
 		chRepetitionLevel = tree depth of chWriter
 	else
 		Add field ID of chWriter to seenFields
 	end if
 	if chWriter corresponds to an atomic field
 		Write value of current field read by decoder
 		using chWriter at chRepetitionLevel
 	else
 		DissectRecord(new RecordDecoder for nested record 
 				read by decoder, chWriter, chRepetitionLevel)
 	end if
 end while
end procedure
```


```
-- 查询SQL
SELECT DocId AS Id,
	COUNT(Name.Language.Code) WITHIN Name AS Cnt,
	Name.Url + ',' + Name.Language.Code AS Str
FROM t
WHERE REGEXP(Name.Url, '^http') AND DocId < 20 ;


-- 查询结果
Id: 10
Name
	Cnt: 2
	Language
		Str: 'http://A,en-us'
		Str: 'http://A,en'
Name
	Cnt: 0		

-- 输出schema
message QueryResult{
	require int64 Id;
	repeated group Name{
		optional uint64 Cnt;
		repeated group Language{
			optional string Str;
		}
	}
}

-- 算法实现
procedure ConstructFSM(Field[] fields):
for each field in fields:
	maxLevel = maximal repetition level of field
	barrier = next field after field or final FSM state otherwise
 	barrierLevel = common repetition level of field and barrier
 	for each preField before field whose
 			repetition level is larger than barrierLevel:
 		backLevel = common repetition level of preField and field
 		Set transition (field, backLevel) -> preField
 	end for
 	for each level in [barrierLevel+1..maxLevel]
 			that lacks transition from field:
 		Copy transition's destination from that of level-1
 	end for
 	for each level in [0..barrierLevel]:
 		Set transition (field, level) -> barrier
 	end for
end for
end procedure
```

现今很多应用程序都是 数据密集型(data-intensive) 的,而非 计算密集型(compute-intensive)的. 
因此CPU很少成为这类应用的瓶颈,更大的问题通常来自数据量、数据复杂性、以及数据的变更速度.

#### 可靠性、可扩展性、可维护性
可靠性(Reliability): 系统在困境(adversity)(硬件故障、软件故障、人为错误)中仍可正常工作(正确完成功能,并能达到期望的性能水准)
可扩展性(Scalability): 有合理的办法应对系统的增长(数据量、流量、复杂性). 讨论可扩展性意味着考虑诸如“如果系统以特定方式增长,
					  有什么选项可以应对增长？”和“如何增加计算资源来处理额外的负载？”等问题.
可维护性(Maintainability): 许多不同的人(工程师、运维)在不同的生命周期,都能高效地在系统上工作(使系统保持现有行为,并适应新的应用场景)

#### Twitter推送系统
1. 第一版: SQL关联查询
2. 第二版: 提前计算、存储每个用户需要查看的关注的人的推文收件箱
  		  消耗计算: 平均一条推文平均会发送75个关注者,所以每秒4.6K的发推写入,变成每秒345K的写入
  		  缺点: 有的用户有超过3000w的粉丝,他们发推文会导致3000w次的写入!!!
3. 第三版: 两种方法的混合.大多数用户发的推文会被扇出写入其粉丝主页时间线缓存中.但是少数拥有海量粉丝的用户(即名流)
		  会被排除在外.当用户读取主页时间线时,分别地获取出该用户所关注的每位名流的推文,再与用户的主页时间线缓存合并,
		  如第一版所示.

#### 延迟和响应时间的区别
延迟(latency) 和 响应时间(response time)经常用作同义词,但实际上它们并不一样.响应时间是客户所看到的,除了实际处理
请求的时间( 服务时间(service time) )之外,还包括网络延迟和排队延迟.延迟是某个请求等待处理的持续时长,在此期间它处于
休眠(latent) 状态,并等待服务

TODO 表结构的规范化和非规范化

关系模型的一个关键洞察是: 只需构建一次查询优化器,随后使用该数据库的所有应用程序都可以从中受益. 如果你没有查询优化器的话,那么为
特定查询手动编写访问路径比编写通用优化器更加容易--不过从长期看通用解决方案更好

声明式查询语言适合并行执行,命令式查询语言并不是特别合适

#### 两大类存储引擎: 日志结构(log-structured)的存储引擎和面向页面(page-oriented)的存储引擎

面向页面: B树
日志结构: asd

#### SSTables(Sorted String Table)和LSM树

SSTables:
	存储: 按键排序、归档、压缩
	搜索: 存储时按规则分块 + 内存字节扫描.   如果字符长度是定长(通常不是),那么可以用二分查找.因为定长的话可以知道每条记录的分界点
										  (前一条记结束,后一条记录开始的地方)
LSM树的基本思想: 保存一系列在后台合并的SSTables,简单而有效.即使数据集比可用内存大得多,它仍能继续正常工作.由于数据按排序顺序存储,
因此可以高效地执行范围查询(扫描所有高于某些最小值和最高值的所有键),并且因为磁盘写入是连续的,所以LSM树可以支持非常高的写入吞吐量

WAL(write-ahead-log)在MySQL中叫"重做日志"(redo log)

B树有个优点是每个键只存在于索引中的一个位置,而日志结构化的存储引擎可能在不同的段中有相同键的多个副本. 这意味着在实现事务语义时方便
太多了: 在许多关系型数据库中,事务隔离是通过在键范围上使用锁来实现的.

反直觉的是,内存数据库的性能优势并不是因为它们不需要从磁盘读取的事实.即使是基于磁盘的存储引擎也可能永远不需要从磁盘读取,
因为操作系统缓存最近在内存中使用了磁盘块.相反,它们更快的原因在于省去了将内存数据结构编码为磁盘数据结构的开销

列存储适用于OLAP是因为分析类型的查询语句中通常不会像OLTP一样的拿非常多行,并且OLAP对应的数据仓库中的数据非常之巨大,所以如果用
行存储的话性能简直不敢想象的低,一个查询语句下去可能会把内存占满.  还有就是OLAP常用统计函数,这种情况下列存储在硬盘中的数据摆放都是
顺序的,顺序读比随机读快一个量级(参考4KB顺序读和4KB随机读测速). 还有,列存储压缩数据会比较简单.

列族: 在每个列族中,它们将一行中的所有列与行键一起存储,并且不使用压缩.

列存储索引和行存储索引区别: 行索引中的多个索引(二级索引)的存储将每一行保存在一个地方(聚簇索引或堆文件); 列索引直接存值;

LSM树的思路同样适用于行存储和列存储

## 编码

XML和CSV不能区分数字和字符串. JSON虽然能区分,但是不区分整数和浮点数,而且不能指定精度.
XML和JSON对Unicode字符串有很好的支持,但是它们不支持二进制数据字符编码. 通常会进行Base64后传输,但会增加约33%的数据大小.

PS: 让不同的组织达成一致的难度超过了其它大多数的问题

Thrift的CompactProtocol(三种二进制协议中的一种)还算有趣,虽然都是些别的地方已经有的编码思路
Protobuf在对数组|列表进行编码时和其它协议不一样,它是通过重复字段的重复标记来实现的. 它有个好处,可以将单值改为多值字段,它是兼容的
Avro是一种之前没有见过的编码思路. 附一张图: https://s2.loli.net/2022/06/02/FclZpVqRtP8SiX6.png

我的看法: 越是扣字节的编码兼容起来越麻烦(Avro),但性能非常好

SOAP优点: 给Producer/Consumer双方提供了契约(对于微服务来说很喜欢这个,有专门的契约测试)、安全调用、事务(两阶段)

本地函数调用和RPC的区别: 可预测性、异常原因、幂等、执行时间、数据编码(大对象)、代码优化(我自己想到的)

## 分布式数据

复制数据的变更算法: 单领导者、多领导者、无领导者

同步复制的优点是,从库保证有与主库一致的最新数据副本.如果主库突然失效,我们可以确信这些数据仍然能在从库上上找到.
缺点是,如果同步从库没有响应(比如它已经崩溃,或者出现网络故障,或其它任何原因),主库就无法处理写入操作.主库必须
阻止所有写入,并等待同步副本再次可用

因此,将所有从库都设置为同步的是不切实际的：任何一个节点的中断都会导致整个系统停滞不前.实际上,如果在数据库上启用
同步复制,通常意味着其中一个跟随者是同步的,而其他的则是异步的.如果同步从库变得不可用或缓慢,则使一个异步从库同步.
这保证你至少在两个节点上拥有最新的数据副本：主库和同步从库.

​通常情况下,基于领导者的复制都配置为完全异步. 在这种情况下,如果主库失效且不可恢复,则任何尚未复制给从库的写入都会丢失. 这意味着即使已经向客户端确认成功,写入也不能保证持久(Durable). 优点是即使所有的从库都落后了,主库也可以继续处理写入.

Redis和Dynamo的高可用实现: Gossip

传统的复制型关系数据库系统都将关注点放在保证副本的强一致性. 虽然强一致性可以给应用的写入操作提供方便的编程模型,但导致系
统的扩展性和可用性非常受限,无法处理网络分裂的情况

### 单领导者

#### 复制数据的几种方法

基于语句的复制: INSERT、UPDATE、DELETE等都会转发给每个从库. 缺点是: 非确定性函数(rand()、now())在每个副本上会生成不同值;
			   "自增列"会在每个副本上产生不同的效果;
传输预写日志(WAL):  缺点是它记录的东西很底层: 哪些磁盘块中的哪些字节发生了更改. 如果数据库将其存储格式从一个版本更改为另一个
				   版本,通常不可能在主库和从库上运行不同版本的数据库软件
逻辑日志复制(基于行): 
				1. 对于插入的行,日志包含所有列的新值.
				2. 对于删除的行,日志包含足够的信息来唯一标识已删除的行.通常是主键,但是如果表上没有主键,则需要记录所有列的旧值.
				3. 对于更新的行,日志包含足够的信息来唯一标识更新的行,以及所有列的新值(或至少所有已更改的列的新值).
				  修改多行的事务会生成多个这样的日志记录,后面跟着一条记录,指出事务已经提交.MySQL的二进制日志(当配置为使用基于行
				的复制时)使用这种方法. 如果要将数据库的内容发送到外部系统的话这一点很个用,这种技术被称为"数据变更捕获"(change data capture).
基于触发器的复制: 允许注册在数据库系统中发生数据更改(写入事务)时自动执行的自定义应用程序代码.

#### 异步复制下的数据不一致解决方法

1. 读用户可能已经修改过的内容时,都从主库读；这就要求有一些方法,不用实际查询就可以知道用户是否修改了某些东西.
举个例子,社交网络上的用户个人资料信息通常只能由用户本人编辑,而不能由其他人编辑.因此一个简单的规则是：从主库读取用户自己的档案
,在从库读取其他用户的档案.

2. 如果应用中的大部分内容都可能被用户编辑,那这种方法就没用了,因为大部分内容都必须从主库读取(扩容读就没效果了).在这种情况下
可以使用其他标准来决定是否从主库读取.例如可以跟踪上次更新的时间,在上次更新后的一分钟内,从主库读.还可以监控从库的复制延迟,防
止任向任何滞后超过一分钟到底从库发出查询.

3. 客户端可以记住最近一次写入的时间戳,系统需要确保从库为该用户提供任何查询时,该时间戳前的变更都已经传播到了本从库中.如果当前
从库不够新,则可以从另一个从库读,或者等待从库追赶上来.
   时间戳可以是逻辑时间戳(指示写入顺序的东西,例如日志序列号)或实际系统时钟(在这种情况下,时钟同步变得至关重要；).

4. 如果您的副本分布在多个数据中心(出于可用性目的与用户尽量在地理上接近),则会增加复杂性.任何需要由领导者提供服务的请求都必须
路由到包含主库的数据中心.

  另一种复杂的情况是：如果同一个用户从多个设备请求服务,例如桌面浏览器和移动APP.这种情况下可能就需要提供跨设备的写后读一致性：
如果用户在某个设备上输入了一些信息,然后在另一个设备上查看,则应该看到他们刚输入的信息. 在这种情况下,还有一些需要考虑的问题：
1. 记住用户上次更新时间戳的方法变得更加困难,因为一台设备上运行的程序不知道另一台设备上发生了什么.元数据需要一个中心存储.

2. 如果副本分布在不同的数据中心,很难保证来自不同设备的连接会路由到同一数据中心.(例如,用户的台式计算机使用家庭宽带连接
,而移动设备使用蜂窝数据网络,则设备的网络路线可能完全不同.如果你的方法需要读主库,可能首先需要把来自同一用户的请求路由到同一
个数据中心.

单调读: 如果先前读取到较新的数据,后续读取不会得到更旧的数据. 实现单调读取的一种方式是确保每个用户总是从同一个副本进行读取(不同的用户可以从不同的副本读取).

### 多领导者

应用场景: 在单个数据中心内部使用多个主库很少是有意义的,因为好处很少超过复杂性的代价。

优点: 容忍网络延迟以及部分数据中心的主节点崩溃; 更快(可以让不同的数据中心更靠近不同地方的用户);
缺点: 数据冲突;

由于多主复制在许多数据库中都属于改装的功能,所以常常存在微妙的配置缺陷,且经常与其他数据库功能之间出现意外的反应。
例如自增主键、触发器、完整性约束等,都可能会有麻烦。因此,多主复制往往被认为是危险的领域,应尽可能避免

处理数据冲突: 
1. LWW(last write wins)
2. 保留冲突,用程序或者跟用户提示这个异常

PS: "冲突解决"这个问题在协同软件中经常碰到,我知道的有`operational transformation`和`Conflict-free replicated datatypes`(CRDT)

#### 多主复制拓扑

适用场景：应用程序在断网之后仍然需要继续工作(比如钉钉的在线文档,无网络时本地代表的就是一个数据库). 

多主复制在同时编辑一条数据时处理起来不如单主,"协同编辑"场景就是个例子

几种拓扑图: https://s2.loli.net/2022/06/03/4QwR3mnSDqjdxMX.png

All-to-all: 每个领导者将其写入每个其他领导
Circular: 每个节点接收来自一个节点的写入,并将这些写入（加上自己的任何写入）转发给另一个节点. MySQL只支持这种
Start: 指定的根节点将写入转发给所有其他节点

为了防止无限复制循环,每个写入都被标记了所有已经通过的节点的标识符。当一个节点收到用自己的标识符标记的数据更改时,
该数据更改将被忽略,因为节点知道它已经被处理。

冲突的解决通常适用于单个行或文档层面,而不是整个事务

### 无主复制

Dynamo、Riak、Cassandra、Voldemort都是无主数据库.

发定人数很有讲究,分"松散的法定人数"(人数可能不包含在集群中"主节点")、"法定人数"(人数全是集群节点中的"主节点")

TODO 捕获"此前发生"关系

## 分区

术语: 分区(partition)在MongoDB,Elasticsearch和Solr Cloud中被称为分片(shard),在HBase中称之为区域(Region),
Bigtable中则是表块（tablet）,Cassandra和Riak中是虚节点（vnode), Couchbase中叫做虚桶(vBucket).但是分区(partition)
是约定俗成的叫法

散列似乎和范围检索天生就是相对的.

一种简单处理热Key的方法: 在主键的开始或结尾添加一个随机数,只要一个两位数的十进制随机数就可以将主键分散为100种不同的主键,
从而存储在不同的分区中. 将主键进行分割之后,任何读取都必须要做额外的工作,因为他们必须从所有100个主键分布中读取数据并将
其合并.此技术还需要额外的记录：只需要对少量热点附加随机数;对于写入吞吐量低的绝大多数主键来是不必要的开销.因此,您还需要
一些方法来跟踪哪些键需要被分割

#### 两种用二级索引对数据库进行分区的方法: 基于文档的分区(document-based)和基于关键词(term-based)的分区

基于文档的分区: 每个分区是完全独立的. 每个分区维护自己的二级索引,仅覆盖该分区中的文档。它不关心存储在其他分区的数据。无
			  论何时您需要写入数据库（添加,删除或更新文档）,只需处理包含您正在编写的文档ID的分区即可。出于这个原因,文档
			  分区索引也被称为本地索引（local index）.
			  
			  它被广泛使用在: MongoDB,Riak,Cassandra,Elasticsearch,SolrCloud和VoltDB

			  缺点: 如果同时使用多个二级索引做条件时性能不太行.

基于关键词的分区: 覆盖所有分区数据的全局索引,而不是给每个分区创建自己的次级索引（本地索引）. 当然,全局索引也会进行分区的.
				
			  缺点: 写入速度较慢且较为复杂,因为写入单个文档现在可能会影响索引的多个分区（文档中的每个关键词可能位于不同的分
			  区或者不同的节点上）;   它还需要分布式事务...

			  支持的数据库: Riak、Oracle
			  
PS: HBase为了减少实现的复杂度而放弃了次级索引

#### 再平衡

固定数量的分区: 创建比节点更多的分区,并为每个节点分配多个分区;  即: 分区数固定,节点再平衡负责的分区.
			   例: 运行在10个节点的集群上的数据库可能会从一开始就被拆分为1,000个分区,因此大约有100个分区被分配给每个节点. 此时
			   集群中加一个节点,那么之前的10个节点会匀一些分区给新节点
动态分区: 当分区增长到超过配置的大小时,会被分成两个分区,每个分区约占一半的数据; 如果大量数据被删除并且分区缩小到某个阈值以下,则可
		 以将其与相邻分区合并; 

		 优点: 动态分配负载
按节点比例分区: 每个节点具有固定数量的分区。在这种情况下，每个分区的大小与数据集大小成比例地增长，而节点数量保持不变，但是当增加节
		       点数时，分区将再次变小。由于较大的数据量通常需要较大数量的节点进行存储，因此这种方法也使每个分区的大小较为稳定

"全自动重新平衡"可以很方便,因为正常维护的操作工作较少.但是,这可能是不可预测的.再平衡是一个昂贵的操作,因为它需要重新路由
请求并将大量数据从一个节点移动到另一个节点.如果没有做好,这个过程可能会使网络或节点负载过重,降低其他请求的性能

#### 请求路由

分区与节点之间的关系变动后,需要一个机制让客户端以及所有节点都知道这个状态,不然一个Key请求过来到底如何精准路由是个问题.

有"统一配置中心"、"共识"两个思路.

统一配置中心: Zookeeper
共识: 流言协议(gossip protocol)、Raft、Zab、Paxos

Cassandra和Riak在节点之间使用流言协议（gossip protocol）来传播群集状态的变化。请求可以发送到任意节点，该节点会转发到包含所请
求的分区的适当节点。这个模型在数据库节点中增加了更多的复杂性，但是避免了对像ZooKeeper这样的外部协调服务的依赖。




===== 分割线 =====


分布式搜索引擎中的服务树的概念(目的是为了能够横向扩展,每台示例能够执行任务树中的一个子节点树)
Java内置的序列化性能在现在的版本中还是不是个问题???
Eureka是AP的??? http://jasonwilder.com/blog/2014/02/04/service-discovery-in-the-cloud/

Cassandra和Riak采取不同的方法：他们在节点之间使用流言协议(gossip protocol) 来传播群集状态的变化. 请求可以发送到任意节点,
该节点会转发到包含所请求的分区的适当节点(图6-7中的方法1).这个模型在数据库节点中增加了更多的复杂性,但是避免了对像ZooKeeper
这样的外部协调服务的依赖.

ACID中的C的概念: 对数据的一组特定陈述必须始终成立,即"不变量". 所以这个C是应用程序的属性,不是数据库的属性

在CouchDB,Datomic和LMDB中使用另一种方法.虽然它们也使用B树,但它们使用的是一种仅追加/写时拷贝(append-only/copy-on-write) 的变体,它们在更新时不覆盖树的页面,而为每个修改页面创建一份副本.从父页面直到树根都会级联更新,以指向它们子页面的新版本.任何不受写入影响的页面都不需要被复制,并且保持不变.
使用仅追加的B树,每个写入事务(或一批事务)都会创建一颗新的B树,当创建时,从该特定树根生长的树就是数据库的一个一致性快照.没必要根据事务ID过滤掉对象,因为后续写入不能修改现有的B树；它们只能创建新的树根.但这种方法也需要一个负责压缩和垃圾收集的后台进程.

并发问题的测试是很难的,因为它们通常是非确定性的 —— 只有在倒霉的时机下才会出现问题.  哈哈

可序列化级别大部分使用了三种技术:
1. 字面意义上地串行顺序执行事务
	1.1. Redis;  
2. 两相锁定(2PL, two-phase locking),几十年来唯一可行的选择
	2.1. 读时阻塞写,写时阻塞读和写; 其实就是排他锁和共享锁
3. 乐观并发控制技术,例如可序列化的快照隔离
	3.1 乐观锁是一种
	3.2 序列化快照隔离(serializable snapshot isolation)非常有趣,它是根据"快照读隔离"所带来的"幻读"问题而提出的解决办法,
		即"记录每个活跃事务所读取的记录并在其它事务提交后标记被影响到的记录已经过时",听起来就是件开销非常大的事. 它与SPL相比
		的优点就是不需要阻塞. 缺点是对于长时间读取和写入数据的事务很可能会发生冲突并中止更加敏感.


另外,现代的存储过程放弃了PL/SQL而是用了现有的编程语言(Datomic使用Java或Clojure),Redis的Lua也是存储过程. 
"Gap Lock+Next Key Lock"是一个简易的"谓词锁"(predicate lock)实现: 适用于数据库中尚不存在,但将来可能会添加的
对象(幻象).如果两阶段锁定包含谓词锁,则数据库将阻止所有形式的写入偏差和其他竞争条件,因此其隔离实现了可串行化


装有良好软件的个人计算机通常要么功能完好,要么完全失效,而不是介于两者之间. 在分布式系统中,我们不再处于理想化的
系统模型中,我们别无选择,只能面对现实世界的混乱现实.而在现实世界中,各种各样的事情都可能会出现问题.

当错误处理策略由简单的放弃组成时,一个大的系统最终会花费大量时间从错误中恢复,而不是做有用的工作

异步网络具有无限的延迟限制

ISDN网络(电路交换网络)属于同步、固定带宽、频率运行(每秒4000帧,每帧16bit),它可以保证"有限延迟"; 而以太网和IP是分组交换协议,
它们是可以从队列中获得,它们针对"突发流量"进行了优化


# Dynamo

解决数据冲突会带来两个额外问题: 何时解决? 谁来解决?
Dynamo针对的主要是需要"永远可写"数据仓库的应用,并且它的"延迟敏感"使得它不能像Chord和Pastry一样使用多跳路由.  
Dynamo可以被描述为: 一个零跳(zero hop)分布式哈希表(DHT)
在发生故障并且存在并发更新的场景下,版本会发生分叉
vector clock有一个潜在问题: 如果有多个节点先后参与同一个对象的写操作,那这个对象的clock vector会变得很长. 但在实际中这
不太可能发生,因为写操作只会发生在perference list(存key的值的物理节点列表)的前N个节点中的一个来执行. 只有在网络分裂或多台
服务器挂掉的情况下,写操作才可能由非preference list前N个节点来执行,导致vector clock变长. Dynamo采用了一种vector clock截断
方案: 另外保存一个和(node, counter)对应的时间戳,记录对应的节点最后一次更新该记录的时间.当vector clock里的(node, counter) 数量达到一个阈值(例如,10)时,就删除最老的一项. 这样会带来无法精确判断部分后代的因果关系的问题.
Merkle Tree缺点: 每当有节点加入或离开系统时,一些key range会变,因此对应的tree需要重新计算
判断不可达: 节点B只要没有应答节点A的消息,A就可以认为B不可达,即便B可以应答C的消息
TODO 去中心化故障检测协议使用简单的 gossip 风格协议,使得系统内的每个节点都可以感知 到其他节点的加入或离开
写请求由preference list内的前N个节点中的任意一个参与者,好处是可以保证写入的顺序化,坏处是会导致不均匀的负载分布,损害SLA
为了解决这个问题,preference list内的所有N个节点都可以参与写操作,而且为一个写操作之前通常有一个读操作,因此写操作的参与者 
都选择为: 前一次读操作返回最快的那个节点,这个信息存储在读操作返回的上下文中

几个不同的使用场景做出不同处理:
1. 业务逻辑相关的: 每个数据都会复制到不同的节点上,发生版本冲突时由应用执行自己的reconciliation逻辑. 购物车服务就是这种场景--应用
				  自己来合并冲突的购物车版本
2. 基于时间戳的: 就是LWW(Last Write Wins). 维护用户Session信息适用于这种场景
3. 高性能读取引擎: 读多写少,一般配置R为1,W为N,保证数据权威(可用)性. 维护产品目录、促销商品服务会用到这种配置 

两种会出现数据版本分叉的情况:
1. 节点失败、数据中心故障或网络分裂等故障场景
2. 同一数据对象的大量并发写操作,不同节点都在coordinating操作

并发写数据上升通常都是自动化客户端程序(busy robots)导致的,极少是人(的应用)导致的.

客户端驱动或服务端驱动的 Coordination:

服务端驱动:
  客户请求会通过负载均衡器均匀地分发给哈希环上的所有节点.每个节点都可以作为读请求的coordinator,而写操作的coordinator必须
由key的preference list里面的节点才能充当.有这种限制是因为,preference list中的这些节点被赋予了额外的职责：创建一个
新的版本戳(version stamp),在因果关系上包含被它的写操作更新的版本.注意,如果Dynamo的版本化方案使用的是物理时间戳
(physical timestamps),那任何节点都可以coordinate写操作.  (大西瓜注: 钱在这个时候的作用非常大,比如谷歌用的原子钟+NTP)

客户端驱动:
  在这种方式中,客户端应用使用库(library)在本地执 行请求 coordination.每个客户端定期地随机选择一个 Dynamo 
节点,下载它的系统成员 状态(Dynamo membership state)的当前视图(current view).有了这个信息,客户端 就可以
知道任何 key 对应的 preference list 由哪些节点组成.

  读请求可以在客户端节点(client node)coordinate,因此如果请求是被负载均衡器随机 分给一个 Dynamo 
节点,那这种方式可以避免额外的网络转发跳数.写操作或者转发给 key 对应的 preference list里面的一个节点,
或者,如果使用的是基于时间戳的版本化方式 ,可以在本地 coordinate.

 客户端驱动的一个重要优势是：不再需要一个负载均衡器才能均匀地分发客户负载. 在存储节点上近乎均匀分布的 key,暗含了
 (implicitly guaranteed)负载的均匀分布.

  显然,这种方式的效率取决于客户端侧的成员信息的新鲜程度(how fresh the membership information).当前,每个客户端
会每隔10s随机地轮询(poll)一个Dynamo节点,获取成员更新(membership updates).这里选用pull而不是push
模型是考虑前者在大 量客户端的情况下可扩展性更好,而且相比于客户端侧,只需在服务端侧维护很少的状态信 息.

  然而,在最差的情况下,客户端的 membership 信息会有 10s 的脏数据. 因此,如果客户端检测到它的成员表
(membership table)过期了(例如,当一些成员不可 达的时候),它会立即更新它的成员信息

它的读时修复可以解决线性一致性和法定人数下因为网络延迟原因导致的客户端读取数据不一致(不正确)问题.
Writer向Replica1、Replica2、Replica3写入数据,但由于网络原因,Reader1和Reader读取数据时只有Replica1写好了最新值1,
其余的两个节点还是旧值0,Reader1读到了Replica1,Reader2读到了Replica2、Replica3,此时Reader1和Reader2的值不一致.

读时修复会在读取数据给应用之前同步执行修复.

顺序一致性和线性一致性的区别是什么?

CAP相关: https://web.archive.org/web/20220126012236/https://zhuanlan.zhihu.com/p/55053121

虽然因果是个很重要的概念,但实际上如果跟踪所有的因果关系是不切实际的. 
兰伯特时间(Lamport clock)适合事后确定全序关系

全序广播非正式地讲需要满足两个安全属性:
1. 可靠交付:
没有消息丢失,如果消息被传递到一个节点,它将被传递到所有节点

2. 全序交付:
消息以相同的顺序传播给每一个节点

全序网路在机器出现故障时也要保证机器恢复后消息能够及时通过并送达

分布式共识常见问题场景:
1. 领导选举
2. 原子提交

2PC都会有存疑事务的问题

纠删码允许比完全复制更低的存储开销以恢复丢失的数据. 这些技术与RAID相似,可以在连接到同一台机器的多个磁盘上提供冗余; 区别在于
在分布式系统中,文件访问和复制是在传统的数据中心网络上完成的,没有特殊的硬件

"OLAP"和"OLTP"和"批处理工作流"和"流处理"的作用是不同的

在使用分布式存储的时候应当尽量避免使用排序

Spark、Flink、Tez是用来解决MapReduce中的"数据连接"的解决方案在一些比较频繁遇到的场景下的一些缺点(是MapReducer模型导致的,所以
是通病)而新开发出来的,它们的设计方式有很多区别,但有一个共同点: 把整个工作流作为单个作业来处理,而不是把它分解为独立的子作业

"批处理"的输入数据有界,所以它总是知道自己是什么时候完成了一个作业

为了区分不同的消息系统,需要思考两个问题:
1. 如果生产者发送消息的速度比消费者能够处理的速度块会发生什么
2. 如果节点崩溃或暂时脱机,会发生什么情况

保持多数据库系统的数据同步问题解决方法可以用"双写",但"双写"还需要引入2PC(有可能"单失败"),所以CDC(change data capture)出来了

事件在用于表示用户的操作意图时是不可压缩的,而用于表示因为操作而发生的状态更新时是可以压缩的

在事件朔源系统中真正删除数据是非常苦难的,因为副本可能存在于很多地方. 删除更多的是“使取回数据更困难”,而不是“使取回数据不可能”.

【复合事件处理】
CEP(complex,event processing)是为分析事件流而开发出的一种方法,CEP系统通常使用高层次的声明式查询语言,比如SQL,或者图形用户
界面,来描述应该检测到的事件模式.这些查询被提交给处理引擎,该引擎消费输入流,并在内部维护一个执行所需匹配的状态机.当发现匹配时,
引擎发出一个复合事件(complex event)(因此得名),并附有检测到的事件模式详情.  在这些系统中,查询和数据之间的关系与普通数据库
相比是颠倒的.通常情况下,数据库会持久存储数据,并将查询视为临时的：当查询进入时,数据库搜索与查询匹配的数据,然后在查询完成时丢
掉查询. CEP引擎反转了角色：查询是长期存储的,来自输入流的事件不断流过它们,搜索匹配事件模式的查询

使用流处理的另一个领域是对流进行分析. CEP与流分析之间的边界是模糊的,但一般来说,分析往往对找出特定事件序列并不关心,
而更关注大量事件上的聚合与统计指标. Apache Storm,Spark Streaming,Flink都是针对分析设计的

函数幂等同样可以实现"恰好一次语义",即: 再次写入该值,只是用同样的值替代

衍生数据与分布式事务最大的区别在于: 事务系统通常提供"线性一致性",这包含有用的保证,例如"读之已写". 另一方面,衍生数据系统通常是异步
更新的,因此默认不会提供相同的时序保证(即不保证"线性一致性")

在Lambda架构中建议并行运行两个不同的系统: 批处理系统和独立的流系统. 流处理器消耗事件并快速生成对视图的近似更新,批处理器稍后将
使用同一组事件并生成衍生视图的更正版本. 这个设计背后的原因是批处理更简单,因此不易出错,而流处理被认为是不太可靠和难以容错的.
而且,流处理可以使用快速近似算法,而批处理使用较慢的精确算法.

违反及时性,"最终一致性"; 违反完整性,"永无一致性"

系统自我验证、自我审计值得投资


 


