GFS、MapReduce、BigTable

Chubby    ==> ZooKeeper
GFS 	  ==> HDFS
BigTable  ==> HBase
MapReduce ==> Hadoop

## GFS 

https://kb.cnblogs.com/page/174130/、https://mr-dai.github.io/gfs/

客户端不缓存文件数据,但会缓存"元数据",直到客户端缓存信息过期或者文件被重新打开时才会重新从master那询问

chunckserver不需要缓存文件数据是因为chunk被存储为本地文件,Linux提供的OS层面的buffer缓存已经保存了频繁访问的文件

GFS将数据流的传输进行了针对网络拓扑的优化: 每个机器都会尝试推送数据到网络拓扑中最近的其他目标机器(用IP地址准确预估"距离")
在没有网络拥挤的情况下,传输B个字节到R个副本的理想耗时是B/T+RL,T是网络吞吐量,L是在机器间传输字节的延迟. 所以假如网络连接是典型
的100Mbps(T),L小于1ms,因此1MB数据流大约耗时80ms

master主要存储三种类型的元数据：文件和chunk的命名空间，从文件到chunk的映射，每个chunk副本的位置

四种状态: 一致、不一致、defined、undefined

GFS通过版本侦测机制踢除某个副本因为机器故障而执行异常的副本

GFS是按照标准的文件API来要求它自己的,所以标准文件API不提供的能力它也不提供. 比如: 当一个文件正在被写入时,它依然可以被另外一个线程读,
所以完全可能会存在读到没有写入完全的数据(标准文件API也不提供这个隔离的能力)

比较严谨的程序会使用各种方法来避免此问题，比如先写入临时文件，写入结束时才原子的重命名文件，此时才对读线程可见.
或者在写入过程中不断记录写入完成量，称之为checkpoint，读线程不会随意读文件的任何位置，它会判断checkpoint（哪个偏移之前的数据是写入完全的，也就是defined），checkpoint甚至也可以像GFS一样做应用级别的checksum。这些措施都可以帮助reader只读取到defined区域

快照和备份的区别: 快照是数据存储的某一刻的状态(元数据)记录; 备份则是数据存储的某一时刻的副本.
理论上，某个地址的数据发生变化前做过快照的话，发生变化时会锁定物理单元不能改写。所以如果发生“最初数据不存在了”，意味着要么没做过快
照，要么快照机制已经被破坏了，自然就无法恢复了。

每当授予一个新的租赁给某个chunk,都会增长chunk版本号并通知各副本. master和这个副本都持久化记录新版本号. 这些都是在写操作被处理
之前就完成了。如果某个副本当前不可用，它的chunk版本号不会被更新。master可以侦测到此chunkserver有旧的副本，因为chunkserver重启
时会汇报它的chunk及其版本号信息。如果master看到一个比自己记录的还要高的版本号，它会认为自己在授予租赁时发生了故障，继而认为更高
的版本才是最新的。

GFS会在做快照的时候释放所有租赁,同时引入"命名空间锁"来保证并发的执行以及某些点的串行

GFS虽然看似是文件目录结构存储形式,但实际上完全不是. 它不会维护维护文件目录结构,以及没有inode信息要维护,不支持listFile,创建
/root/daxigua时也不需要申请/root的写锁(但会申请读锁)

为了防止死锁,一个操作必须按照顺序来申请锁: 首先按命名空间的层级顺序,在相同层级再按字典排序

当一个文件被删除时,master并不会立即打印删除操作的日志,然而不会立刻回收资源,仅仅将文件重命名为一个隐藏的名字,包含删除时间戳.
在master对文件系统命名空间执行常规扫描时，它会删除任何超过3天的隐藏文件（周期可配）。在那之前此隐藏文件仍然能够被读，
而且只需将它重命名回去就能恢复。当隐藏文件被删除时，它才在内存中元数据中被清除，高效的切断它到自己所有chunk的引用。

GFS、TFS、Haystack都是自己定义一个逻辑存储块,目的是为了将用户的数据转换为适应本系统的物理存储格式,同时也防止用户直接操作存储细节

普通Linux文件系统上，固定大小的文件+预分配空间+合理的文件总数量+合理的目录结构等等，往往是保证I/O性能的常用方案。
所以必须有个明确的逻辑存储单元。

GFS、Haystack中都有协调组件,它们的作者都希望能将其简化,原因是: 人如果有两个大脑那么很多事情会很麻烦. 
如果协调器有两个，那客户端听谁的、两个协调器信息是否需要同步、两个协调器在指定策略时是否有资源竞争. 
GFS的只有当元数据操作的日志已经成功flush到本地磁盘和所有master副本上才会认为其成功,master副本只参与	"备份"
GFS有Shadow Master(只读的),但只是作为容灾备用,不会在线参与协调,它读取的是master副本的操作日志

在存储时,Haystack的客户端直接面对所有存储组件(所有主备物理卷,分别手动写入),GFS的客户端只面对主DataServer

Haystack用"新增+删除"来模拟修改的,因为它需要承担图片的真实文件中的存储格式和检索等责任. 
而GFS把这些交给用户,所以如果用户将100KB的文件原地修改为一个101KB的文件,那么最后面那1KB文件就会破坏掉原先的数据

GFS的愿景是给用户提供一个无限容量、放心使用的硬盘,快速的存取文件,而不是特定于某种特殊场景(小文件或图片)

GFS为什么会有租赁机制而Haystack和TFS没有: 编程界面的问题. Haystack和TFS的客户端中面对的是一个图片,只需要将图片存到服务端然后返回
图片UUID就行了,其它副本存储成ABC、BAC、CBA、BCA都没有问题. 而GFS是按照文件系统来设计的,这种情况也就是之前所说的undefined问题

GFS的实现HDFS中的HA部分是基于Zookeeper实现的,一个Active节点,多个Standby节点,Active节点失败后所有Standby节点会竞争. 为了防止
"脑裂"问题,FailoverController会通知Standby节点强制下线之前断连的Active节点(可以配置脚本)

Q: GFS 是怎么确定最近的 Replica 的位置的？
A: 论文中有提到是基于备选服务器的IP来判断距离的,我判断是人为控制相近的IP(网段作为一个单位,IP作为一个单位,或者更新)实际位置相近.
   当然,"wifi探针"能实现更实际的"测距"功能,"AP工作模式"更是能返回连接者的MAC地址

## Face Book Haystack

http://web.archive.org/web/20190603070105/http://www.importnew.com/3292.html

架构范式: 元数据总控 + 分布式协调调度 + 分区存储

背景: 以前的存储图片的方案(基于NFS和NAS),需要提供每秒1 million图片的能力. 以前方案存在一个问题: 以前方案中因为元数据查询而导致
了过多的磁盘操作. Haystack竭尽全力的减少每个图片的元数据,让其能够全部在内存中处理有所元数据

为什么不构建成一个类似GFS的系统: Fack Book大部分用户数据都存储在Mysql数据库,文件存储主要用于开发工作、日志数据以及图片.
NAS设备其实为这些场景提供了性价比很好的方案. 此外,补充了hadoop以供海量日志数据处理. 面对图片服务的long tail问题(不常用图片突然
并发),Mysql、NAS、Hadoop都不太合适.  基于NAS的方案,一个图片对应一个文件,且每一个文件需要至少一个inode,这已经占了几百byte,无法将
所有inode都存到缓存中,所以需要一个定制系统.  

每个数据只会写入一次、读操作频繁、从不修改、很少删除
NFS很多元数据（比如文件权限），是无用的而且浪费了很多存储容量
更大的性能消耗在于文件的元数据必须从磁盘读到内存来定位文件,访问元数据就是吞吐量瓶颈所在
即使是压缩了元数据,每个图片存储为一个文件还是会导致元数据过多,难以被缓存. 所以将多个图片存储在单个文件中,控制文件个数,维护大型文件
读取单个照片就需要好几个磁盘操作: 一个（有时候更多）转换文件名为inode number，另一个从磁盘上读取inode，最后一个读取文件本身
句柄打开文件 
hadoop以供海量日志数据处理   
基于NAS的方案,一个图片对应到一个文件,每个文件需要至少一个inode,这块需要的内存就很昂贵  Haystack的取舍是: 接受
long tail的磁盘IO这个现实,但会尽量减免除了真实访问图片数据之外的其它操作,并压缩文件系统元数据的空间,将所有元数据都放到内存中  
一个文件存储多个文件,个数可控  
Haystack在设计时是假设在没有CDN的环境   
http://<CDN>/<Cache>/<Machine id>/<Logical volume, Photo>  
大部分图片在上传之后很快就会被频繁访问 
内存中维护的数据结构[key: {needle.key,needle.alternate_key},value: {needle.flag,needle.data,needle.size,needle.offset}]
`needle.alternate_key`是图片副本的标识,我认为的实际数据结构: Map<long/*needle.key*/,Map<int/*alternate_key*/,Object>>


## MapReduce




## BigTable

https://dblab.xmu.edu.cn/post/google-bigtable/、https://niceaz.com/2019/03/24/bigtable/

最初是为了用于Google Analytics

用来管理结构化数据的分布式存储系统

Bigtable将数据视为普通字符串(简单数据模型)且不关注其内容

Bigtable是一个稀疏的、分布式的、持久化的多维排序字典(map).该字典通过行键(row key)、列键(column)和时间戳(timestamp)索引,字典中的每个值都是字节数组`(row:string, column:string, time:int64) -> string`.  这是在调研了类BigTable系统的各种
潜在用途后决定采用的数据模型. 驱动做出部分设计决策的案例是: 假设我们想要持有一份可在很多项目中使用的大量web页面和相关信息的
副本,我们称这个副本构成的特殊的表为Webtable.

|             | contents:                                       | anchor:cnnsi.com | anchor:my.look.ca |
|-------------|-------------------------------------------------|------------------|-------------------|
| com.cnn.www | <html>... <- t3
<html>... <- t5
<html>... <- t6 | "CNN" <- t9      | "CNN.com" <- t8   |

上面的`contents`和`anchor`都是列族,并且`contents`列族中的数据都是经过反转的,这样可以使得"域名相同的网页被分组到连续的行中"从而被分配到临近的Tablet(针对表分区后的基本单位)中.

列族是访问控制（access control）的基本单位. 使用格式命名：列族名:限定符. 访问控制以及磁盘和内存统计都在列族级别执行

Bigtable中的单元格可以包含相同数据的不同版本，这些版本使用时间戳(64位)索引. 
在上面的例子中为“content：”列中存储的爬取到的页面设置的时间戳为：该版本的页面被爬取到的实际时间。垃圾回收机制允许我们仅保留每个页面的最近3个版本(可配)

Bigtable支持单行事务; 可以作为MapReduce的IO;

为了减少memtable的读写冲突(主要是读取memtable的冲突),采用了'copy-on-write'来允许读操作和写操作同时进行








